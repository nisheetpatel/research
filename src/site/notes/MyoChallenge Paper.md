---
{"dg-publish":true,"permalink":"/myo-challenge-paper/","created":"","updated":""}
---


There are two key challenges in building models for complex and skillful motor tasks. First, we need physiologically detailed musculoskeletal models. Second, we need interpretable yet powerful algorithms: interpretable, in that they allow us to explore the core principles of learning to investigate emergent dynamical movement motifs, and powerful, in that they are able to tackle very high-dimensional problems such dexterous manipulation directly by muscles. While existing in-silico frameworks such as OpenSim contain such models, they are slow and lack the capability for the complex interaction with the physical world outside the agent’s body. Fortunately, we now have such musculoskeletal environments and tasks simulated with the MuJoCo physics engine, thanks to MyoSuite, which is 4000x faster than other simulators.

And while motor neuro literature is rich in outlining the required principles, we can't possibly hope for the proposed toy models to succeed on such a problem. Luckily, with the confluence of ideas from stochastic optimal control and the general-purpose framework provided by reinforcement learning (RL), in this work, we introduce a procedure that utilizes the principles that have been proposed by neuroscientists since the 90s. We refer to it as Static to Dynamic Stability (SDS), which as the name suggests, learns stable solutions at several desired points in the trajectory and gradually relaxes stability constraints to learn a dynamical solution that finds potentially unstable points between the stable ones and even relaxes the stability constraint in its entirety eventually. This has roots that go as far back as Bizzi's equilibrium point hypothesis (cite) but is also related to and inspired by modern RL and imitation learning concepts such as Reference State Initialization (cite). We pair it with a recurrent version of a popular RL algorithm, Proximal Policy Optimization (PPO), which also has some of its roots in the normative approach of stochastic optimal control (cite Todorov 2004, Wolpert). Our model beat all baselines by a landslide and won the NeurIPS 2022 competition track for the Baoding Balls task MyoSuite. 

We believe that building such models will bring about new advents in studying motor control. In future work, we plan to investigate what aspects of the world are encoded in memory throughout learning, for instance, by analyzing how the recurrent dynamics change over the course of curriculum learning and with noise or domain randomization.


Additional points that could be made and used here (Redish 2015, Psychology Today):
- Process models and normative models are two sides of the same coin:
	- _What is the optimal choice given the constraints of the process?_ and _What are the assumptions underlying a given optimization result?_
- In neuroscience, this debate is usually phrased in terms of Marr’s three levels – the _computational_ level is the problem you are trying to solve (how do I get from Minneapolis to San Francisco?), the _algorithmic_ level is the way that you are solving the problem (am I flying, walking, or driving?), and the _implementation_ level is the way that you are implementing that algorithm (am I driving your car or mine?) While it is useful to think about all three of these levels for any question, the levels interact.