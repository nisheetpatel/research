---
{"dg-publish":true,"permalink":"/serotonin-m1-a/","created":"","updated":""}
---

[[Serotonin immersion grant\|Serotonin immersion grant]]

# 5HT immersion story

## Meeting with Alex
two kinds of learning:
- self-supervised
- prediction learning (rewards, etc)
	
Transformers are one way to 
pre-train system by having system predict next input

- Dopamine story is the only good theory of RL.
- For the self-supervised, there is not a good theory. Error signals are very high dimensional. Not clear why you need neuromodulators for this.
attention: we're thinking of too much

- maybe we tend ot simplify too much

There's a selection problem, which is key.
2nd problem:
when in sensorimotor loop
first prediction is what your actions will do
2nd prediction is what the world will do

when zach talk about transparency, he only talks about agent themselves, not outside world.
often, most important

come up with task. which system should I try to predict?
is serotonic just about recuiting attention resources?

### crazy project idea
in a NN, small nuclei that can send proj to whole net. Connection properties. restrictions of how they connect, and hwat they do...  something may not just be an input inthe sum, they may have different roles - multiplicative. 
train system without and with them. See whether and which one is better. How do they split up the job between the diferent neuromodulators. global learning systems

##### bayesian stuff
- Rao and Ballard 1999 predictive coding nature neuro paper
- sophie deneve's Bayesian inference in loopy networks